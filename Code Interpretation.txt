6                   max recall  0.425894 1.000000 396
7              max specificity  0.959159 1.000000   0
8             max absolute_mcc  0.665193 0.178134 340
9   max min_per_class_accuracy  0.858816 0.597015 155
10 max mean_per_class_accuracy  0.831966 0.603885 192

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`
H2OBinomialMetrics: deeplearning
** Reported on validation data. **
** Metrics reported on full validation frame **

MSE:  0.1145423
RMSE:  0.3384409
LogLoss:  0.3856536
Mean Per-Class Error:  0.4856066
AUC:  0.6271038
Gini:  0.2542077

Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
        0    1    Error       Rate
0      28  857 0.968362   =857/885
1      16 5595 0.002852   =16/5611
Totals 44 6452 0.134390  =873/6496

Maximum Metrics: Maximum metrics at their respective thresholds
                        metric threshold    value idx
1                       max f1  0.560294 0.927630 378
2                       max f2  0.426638 0.969654 396
3                 max f0point5  0.574981 0.890540 375
4                 max accuracy  0.560294 0.865610 378
5                max precision  0.959081 1.000000   0
6                   max recall  0.426638 1.000000 396
7              max specificity  0.959081 1.000000   0
8             max absolute_mcc  0.833648 0.141579 197
9   max min_per_class_accuracy  0.860137 0.588701 158
10 max mean_per_class_accuracy  0.835048 0.596087 195

Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`


Scoring History: 
             timestamp          duration training_speed   epochs
1  2018-11-17 18:03:23         0.000 sec             NA  0.00000
2  2018-11-17 18:03:33        10.476 sec   2024 obs/sec  0.73276
3  2018-11-17 18:03:43        20.729 sec   2035 obs/sec  1.46842
4  2018-11-17 18:03:53        30.844 sec   2058 obs/sec  2.20205
5  2018-11-17 18:04:03        40.896 sec   2069 obs/sec  2.93156
6  2018-11-17 18:04:13        50.923 sec   2076 obs/sec  3.66541
7  2018-11-17 18:04:24  1 min  1.852 sec   2047 obs/sec  4.39945
8  2018-11-17 18:04:35  1 min 13.140 sec   2015 obs/sec  5.13372
9  2018-11-17 18:04:45  1 min 23.395 sec   2025 obs/sec  5.87089
10 2018-11-17 18:04:55  1 min 33.379 sec   2034 obs/sec  6.59968
11 2018-11-17 18:05:06  1 min 43.384 sec   2042 obs/sec  7.33731
12 2018-11-17 18:05:16  1 min 53.808 sec   2046 obs/sec  8.07154
13 2018-11-17 18:05:35  2 min 13.318 sec   2054 obs/sec  9.54045
14 2018-11-17 18:05:46  2 min 23.459 sec   2055 obs/sec 10.27166
15 2018-11-17 18:05:46  2 min 24.252 sec   2055 obs/sec 10.27166
   iterations       samples training_rmse training_logloss training_r2
1           0      0.000000            NA               NA          NA
2           1  19399.000000       0.33099          0.38065     0.00613
3           2  38875.000000       0.34093          0.39547    -0.05445
4           3  58297.000000       0.32761          0.37238     0.02628
5           4  77610.000000       0.32660          0.36587     0.03232
6           5  97038.000000       0.32565          0.36373     0.03794
7           6 116471.000000       0.32558          0.36533     0.03833
8           7 135910.000000       0.32567          0.36490     0.03779
9           8 155426.000000       0.32573          0.36513     0.03747
10          9 174720.000000       0.32606          0.36372     0.03550
11         10 194248.000000       0.32507          0.36206     0.04134
12         11 213686.000000       0.32588          0.36424     0.03659
13         13 252574.000000       0.32808          0.37275     0.02349
14         14 271932.000000       0.32389          0.35937     0.04828
15         14 271932.000000       0.32588          0.36424     0.03659
   training_auc training_lift training_classification_error
1            NA            NA                            NA
2       0.61718       1.06581                       0.12554
3       0.60267       1.12191                       0.12465
4       0.61710       1.06581                       0.12416
5       0.63886       1.14435                       0.12446
6       0.64532       1.14435                       0.12455
7       0.65079       1.11069                       0.12317
8       0.64858       1.06581                       0.12287
9       0.64495       1.14435                       0.12317
10      0.65327       1.06732                       0.12406
11      0.64487       1.12213                       0.12356
12      0.64228       1.11036                       0.12346
13      0.64217       1.08770                       0.12446
14      0.66310       1.11102                       0.12297
15      0.64228       1.11036                       0.12346
   validation_rmse validation_logloss validation_r2 validation_auc
1               NA                 NA            NA             NA
2          0.34249            0.40506       0.00319        0.60464
3          0.35354            0.42019      -0.06213        0.59024
4          0.33878            0.39403       0.02469        0.61390
5          0.33866            0.39015       0.02535        0.62349
6          0.33792            0.38661       0.02965        0.63318
7          0.33786            0.38998       0.03000        0.63871
8          0.33985            0.39602       0.01851        0.61102
9          0.33887            0.39186       0.02414        0.62603
10         0.33933            0.39068       0.02152        0.62661
11         0.33781            0.38572       0.03029        0.62738
12         0.33844            0.38565       0.02664        0.62710
13         0.34096            0.40147       0.01211        0.61820
14         0.33766            0.38672       0.03112        0.63943
15         0.33844            0.38565       0.02664        0.62710
   validation_lift validation_classification_error
1               NA                              NA
2          1.08648                         0.13624
3          1.05086                         0.13624
4          1.08648                         0.13377
5          1.10429                         0.13408
6          1.15773                         0.13408
7          1.15773                         0.13362
8          0.96180                         0.13393
9          1.15773                         0.13393
10         1.01524                         0.13408
11         1.12317                         0.13424
12         1.14045                         0.13439
13         1.14070                         0.13501
14         1.05849                         0.13424
15         1.14045                         0.13439

Variable Importances: (Extract with `h2o.varimp`) 
=================================================

Variable Importances: 
   variable relative_importance scaled_importance percentage
1        B2            1.000000          1.000000   0.085265
2        A3            0.944757          0.944757   0.080555
3        B3            0.919322          0.919322   0.078386
4      s600            0.907472          0.907472   0.077376
5        B6            0.887641          0.887641   0.075685
6        B1            0.776250          0.776250   0.066187
7        B4            0.744952          0.744952   0.063519
8    Param2            0.675675          0.675675   0.057612
9       BN1            0.544038          0.544038   0.046388
10   Param4            0.524713          0.524713   0.044740
11   Param1            0.513913          0.513913   0.043819
12       A2            0.463047          0.463047   0.039482
13   Param5            0.451765          0.451765   0.038520
14       B5            0.440395          0.440395   0.037550
15 Mix50.50            0.437430          0.437430   0.037298
16       A1            0.410623          0.410623   0.035012
17 Mix40.60            0.403452          0.403452   0.034400
18       B7            0.353002          0.353002   0.030099
19   Param3            0.329642          0.329642   0.028107
> plot(dl.model)
> 
> dl.model.predict <- h2o.predict(dl.model, test.hex)
  |=================================================================| 100%
> dl.result <- as.data.frame(dl.model.predict)
> dl.result
      predict         p0        p1
1           1 0.11687898 0.8831210
2           1 0.12077737 0.8792226
3           1 0.07786045 0.9221396
4           1 0.12757631 0.8724237
5           1 0.07948517 0.9205148
6           1 0.05530291 0.9446971
7           1 0.13548358 0.8645164
8           1 0.13607936 0.8639206
9           1 0.12466241 0.8753376
10          1 0.11309451 0.8869055
11          1 0.13143459 0.8685654
12          1 0.12250752 0.8774925
13          1 0.05103671 0.9489633
14          1 0.11512822 0.8848718
15          1 0.09611747 0.9038825
16          1 0.05014371 0.9498563
17          1 0.05028781 0.9497122
18          1 0.12985995 0.8701401
19          1 0.08099020 0.9190098
20          1 0.10813830 0.8918617
21          1 0.31669030 0.6833097
22          1 0.26737829 0.7326217
23          1 0.11443191 0.8855681
24          1 0.11415933 0.8858407
25          1 0.11455826 0.8854417
26          1 0.07305977 0.9269402
27          1 0.12403361 0.8759664
28          1 0.12757631 0.8724237
29          1 0.07948517 0.9205148
30          1 0.10266950 0.8973305
31          1 0.09165839 0.9083416
32          1 0.11902424 0.8809758
33          1 0.11019240 0.8898076
34          1 0.07706342 0.9229366
35          1 0.32844214 0.6715579
36          1 0.26593861 0.7340614
37          1 0.32682189 0.6731781
38          1 0.10437756 0.8956224
39          1 0.14037714 0.8596229
40          1 0.19568200 0.8043180
41          1 0.08930602 0.9106940
42          1 0.15678498 0.8432150
43          1 0.14764123 0.8523588
44          1 0.15020230 0.8497977
45          1 0.19875194 0.8012481
46          1 0.18578030 0.8142197
47          1 0.18178280 0.8182172
48          1 0.08346605 0.9165339
49          1 0.08384006 0.9161599
50          1 0.04726371 0.9527363
51          1 0.22362116 0.7763788
52          1 0.21683711 0.7831629
53          1 0.10234515 0.8976548
54          1 0.16725220 0.8327478
55          1 0.16637279 0.8336272
56          1 0.10877626 0.8912237
57          1 0.17634879 0.8236512
58          1 0.10135119 0.8986488
59          1 0.33005511 0.6699449
60          1 0.19288113 0.8071189
61          1 0.13529139 0.8647086
62          1 0.06457939 0.9354206
63          1 0.09216306 0.9078369
64          1 0.08046230 0.9195377
65          1 0.13186574 0.8681343
66          1 0.13262995 0.8673700
67          1 0.08116288 0.9188371
68          1 0.23393171 0.7660683
69          1 0.13159754 0.8684025
70          1 0.13126033 0.8687397
71          1 0.09292270 0.9070773
72          1 0.13313169 0.8668683
73          1 0.12644576 0.8735542
74          1 0.09558327 0.9044167
75          1 0.07904896 0.9209510
76          1 0.09304245 0.9069576
77          1 0.10437756 0.8956224
78          1 0.16365269 0.8363473
79          1 0.16033989 0.8396601
80          1 0.19119487 0.8088051
81          1 0.13697795 0.8630220
82          1 0.18887223 0.8111278
83          1 0.13074263 0.8692574
84          1 0.12733793 0.8726621
85          1 0.07536575 0.9246342
86          1 0.12317685 0.8768232
87          1 0.22359190 0.7764081
88          1 0.23970990 0.7602901
89          1 0.22778199 0.7722180
90          1 0.22218241 0.7778176
91          1 0.21174840 0.7882516
92          1 0.20597175 0.7940283
93          1 0.17949026 0.8205097
94          1 0.09221401 0.9077860
95          1 0.19809647 0.8019035
96          1 0.15607443 0.8439256
97          1 0.19053074 0.8094693
98          1 0.16328549 0.8367145
99          1 0.09200868 0.9079913
100         1 0.13129289 0.8687071
101         1 0.18231875 0.8176813
102         1 0.09457491 0.9054251
103         1 0.12401607 0.8759839
104         1 0.17455881 0.8254412
105         1 0.13404754 0.8659525
106         1 0.13422306 0.8657769
107         1 0.09737142 0.9026286
108         1 0.10865095 0.8913491
109         1 0.23896016 0.7610398
110         1 0.15558179 0.8444182
111         1 0.15650446 0.8434955
112         1 0.14764123 0.8523588
113         1 0.17621697 0.8237830
114         1 0.15953155 0.8404684
115         1 0.07755115 0.9224489
116         1 0.07808173 0.9219183
117         1 0.08076093 0.9192391
118         1 0.12403361 0.8759664
119         1 0.10233770 0.8976623
120         1 0.08024097 0.9197590
121         1 0.13035665 0.8696433
122         1 0.26424333 0.7357567
123         1 0.38983156 0.6101684
124         1 0.39323426 0.6067657
125         1 0.25829645 0.7417035
126         1 0.15497454 0.8450255
127         1 0.36189444 0.6381056
128         1 0.42847303 0.5715270
129         1 0.32621254 0.6737875
130         1 0.31378827 0.6862117
131         1 0.33002467 0.6699753
132         1 0.31830151 0.6816985
133         1 0.25928379 0.7407162
134         1 0.25131114 0.7486889
135         1 0.24383777 0.7561622
136         1 0.24738575 0.7526143
137         1 0.15116438 0.8488356
138         1 0.12486320 0.8751368
139         1 0.10107576 0.8989242
140         1 0.07743871 0.9225613
141         1 0.09421764 0.9057824
142         1 0.07787742 0.9221226
143         1 0.07855147 0.9214485
144         1 0.07615673 0.9238433
145         1 0.07651467 0.9234853
146         1 0.10132252 0.8986775
147         1 0.10200228 0.8979977
148         1 0.13143459 0.8685654
149         1 0.12032375 0.8796762
150         1 0.08061829 0.9193817
151         1 0.09647035 0.9035297
152         1 0.05009620 0.9499038
153         1 0.09358907 0.9064109
154         1 0.09069001 0.9093100
155         1 0.08222286 0.9177771
156         1 0.19068104 0.8093190
157         1 0.12546738 0.8745326
158         1 0.13574224 0.8642578
159         1 0.17370757 0.8262924
160         1 0.13524828 0.8647517
161         1 0.16579385 0.8342062
162         1 0.16261247 0.8373875
163         1 0.15895375 0.8410462
164         1 0.15762790 0.8423721
165         1 0.12008686 0.8799131
166         1 0.12323162 0.8767684
167         1 0.12463368 0.8753663
168         1 0.09452128 0.9054787
169         1 0.18568488 0.8143151
170         1 0.09007602 0.9099240
171         1 0.09024505 0.9097550
172         1 0.14872807 0.8512719
173         1 0.13755685 0.8624432
174         1 0.05964936 0.9403506
175         1 0.09165839 0.9083416
176         1 0.11426825 0.8857317
177         1 0.09640113 0.9035989
178         1 0.07761104 0.9223890
179         1 0.11415933 0.8858407
180         1 0.07369660 0.9263034
181         1 0.11455826 0.8854417
182         1 0.11549347 0.8845065
183         1 0.22217312 0.7778269
184         1 0.31812087 0.6818791
185         1 0.31680236 0.6831976
186         1 0.29837790 0.7016221
187         1 0.36230110 0.6376989
188         1 0.33956259 0.6604374
189         1 0.27683416 0.7231658
190         1 0.19766810 0.8023319
191         1 0.07635317 0.9236468
192         1 0.11920119 0.8807988
193         1 0.11426473 0.8857353
194         1 0.10013235 0.8998677
195         1 0.09714449 0.9028555
196         1 0.06713170 0.9328683
197         1 0.11578586 0.8842141
198         1 0.14337033 0.8566297
199         1 0.13558092 0.8644191
200         1 0.12632522 0.8736748
201         1 0.12314823 0.8768518
202         1 0.12569993 0.8743001
203         1 0.13085016 0.8691498
204         1 0.10299056 0.8970094
205         1 0.14690072 0.8530993
206         1 0.29819431 0.7018057
207         1 0.08955996 0.9104400
208         1 0.12485863 0.8751414
209         1 0.12782448 0.8721755
210         1 0.07034718 0.9296528
211         1 0.15115853 0.8488415
212         1 0.06709913 0.9329009
213         1 0.11395889 0.8860411
214         1 0.09216306 0.9078369
215         1 0.10166945 0.8983306
216         1 0.11481545 0.8851845
217         1 0.11575023 0.8842498
218         1 0.08281480 0.9171852
219         1 0.13901491 0.8609851
220         1 0.13965323 0.8603468
221         1 0.13624074 0.8637593
222         1 0.11956352 0.8804365
223         1 0.14337033 0.8566297
224         1 0.10028492 0.8997151
225         1 0.12821813 0.8717819
226         1 0.09534970 0.9046503
227         1 0.10515201 0.8948480
228         1 0.09974129 0.9002587
229         1 0.07894352 0.9210565
230         1 0.11443191 0.8855681
231         1 0.11225628 0.8877437
232         1 0.13965323 0.8603468
233         1 0.09168538 0.9083146
234         1 0.12401131 0.8759887
235         1 0.12237684 0.8776232
236         1 0.11875767 0.8812423
237         1 0.09215254 0.9078475
238         1 0.11508566 0.8849143
239         1 0.17090427 0.8290957
240         1 0.09700615 0.9029939
241         1 0.08067917 0.9193208
242         1 0.06083703 0.9391630
243         1 0.11616030 0.8838397
244         1 0.26277669 0.7372233
245         1 0.30305323 0.6969468
246         1 0.23628159 0.7637184
247         1 0.39671185 0.6032881
248         1 0.29362703 0.7063730
249         1 0.28670183 0.7132982
250         1 0.23167011 0.7683299
251         1 0.17949026 0.8205097
252         1 0.17136109 0.8286389
253         1 0.16708084 0.8329192
254         1 0.09087580 0.9091242
255         1 0.11698522 0.8830148
256         1 0.12447268 0.8755273
257         1 0.11905502 0.8809450
258         1 0.22638683 0.7736132
259         1 0.05074071 0.9492593
260         1 0.11959425 0.8804057
261         1 0.14334460 0.8566554
262         1 0.11156019 0.8884398
263         1 0.13933024 0.8606698
264         1 0.13256888 0.8674311
265         1 0.22778199 0.7722180
266         1 0.19722946 0.8027705
267         1 0.12911684 0.8708832
268         1 0.23019777 0.7698022
269         1 0.10996804 0.8900320
270         1 0.16725220 0.8327478
271         1 0.16552204 0.8344780
272         1 0.16389162 0.8361084
273         1 0.13947669 0.8605233
274         1 0.14035583 0.8596442
275         1 0.20273128 0.7972687
276         1 0.20022764 0.7997724
277         1 0.22726327 0.7727367
278         1 0.07832500 0.9216750
279         1 0.20973316 0.7902668
280         1 0.19022591 0.8097741
281         1 0.15386178 0.8461382
282         1 0.09150244 0.9084976
283         1 0.08884672 0.9111533
284         1 0.08767104 0.9123290
285         1 0.14758338 0.8524166
286         1 0.17301946 0.8269805
287         1 0.08003259 0.9199674
288         1 0.11956352 0.8804365
289         1 0.14337033 0.8566297
290         1 0.13965323 0.8603468
291         1 0.09168538 0.9083146
292         1 0.09162929 0.9083707
293         1 0.13883020 0.8611698
294         1 0.12656840 0.8734316
295         1 0.19444977 0.8055502
296         1 0.13261918 0.8673808
297         1 0.27691230 0.7230877
298         1 0.12770632 0.8722937
299         1 0.08453647 0.9154635
300         1 0.09292270 0.9070773
301         1 0.26909079 0.7309092
302         1 0.07602352 0.9239765
303         1 0.07297958 0.9270204
304         1 0.11432671 0.8856733
305         1 0.24797468 0.7520253
306         1 0.16722370 0.8327763
307         1 0.19215796 0.8078420
308         1 0.30256070 0.6974393
309         1 0.17620681 0.8237932
310         1 0.30055572 0.6994443
311         1 0.13586129 0.8641387
312         1 0.10543927 0.8945607
313         1 0.07433626 0.9256637
314         1 0.09088461 0.9091154
315         1 0.07301228 0.9269877
316         1 0.14113919 0.8588608
317         1 0.09807827 0.9019217
318         1 0.11019240 0.8898076
319         1 0.10848432 0.8915157
320         1 0.08545724 0.9145428
321         1 0.09330736 0.9066926
322         1 0.07736709 0.9226329
323         1 0.09647035 0.9035297
324         1 0.09578081 0.9042192
325         1 0.07305977 0.9269402
326         1 0.09680035 0.9031996
327         1 0.07347393 0.9265261
328         1 0.11743043 0.8825696
329         1 0.21418833 0.7858117
330         1 0.08601718 0.9139828
331         1 0.07682676 0.9231732
332         1 0.08177550 0.9182245
333         1 0.08459381 0.9154062
 [ reached getOption("max.print") -- omitted 10657 rows ]
> h2o.shutdown()
Are you sure you want to shutdown the H2O instance running at http://localhost:54321/ (Y/N)? 
> # examine the dl.result
> summary(dl.result)
 predict         p0                p1        
 0:   81   Min.   :0.04068   Min.   :0.1838  
 1:10909   1st Qu.:0.10057   1st Qu.:0.8169  
           Median :0.13313   Median :0.8669  
           Mean   :0.15157   Mean   :0.8484  
           3rd Qu.:0.18314   3rd Qu.:0.8994  
           Max.   :0.81620   Max.   :0.9593  
> #### Explanation
> # p0 is the probability that 0 is chosen.
> # p1 is the probability that 1 is chosen.
> # predict: is made by applying a threshold to p1
> 
> # List the important variables
> head(as.data.frame(h2o.varimp(dl.model)))
  variable relative_importance scaled_importance percentage
1       B2           1.0000000         1.0000000 0.08526538
2       A3           0.9447575         0.9447575 0.08055510
3       B3           0.9193221         0.9193221 0.07838635
4     s600           0.9074720         0.9074720 0.07737595
5       B6           0.8876407         0.8876407 0.07568502
6       B1           0.7762501         0.7762501 0.06618726
> 
> # Percentage of good quality prediction
> length(dl.result$predict[dl.result$predict=="1"])*100/length(dl.result$predict)
[1] 99.26297
> ################### Method: h2o #####################
> 
> dl.result
      predict         p0        p1
1           1 0.11687898 0.8831210
2           1 0.12077737 0.8792226
3           1 0.07786045 0.9221396
4           1 0.12757631 0.8724237
5           1 0.07948517 0.9205148
6           1 0.05530291 0.9446971
7           1 0.13548358 0.8645164
8           1 0.13607936 0.8639206
9           1 0.12466241 0.8753376
10          1 0.11309451 0.8869055
11          1 0.13143459 0.8685654
12          1 0.12250752 0.8774925
13          1 0.05103671 0.9489633
14          1 0.11512822 0.8848718
15          1 0.09611747 0.9038825
16          1 0.05014371 0.9498563
17          1 0.05028781 0.9497122
18          1 0.12985995 0.8701401
19          1 0.08099020 0.9190098
20          1 0.10813830 0.8918617
21          1 0.31669030 0.6833097
22          1 0.26737829 0.7326217
23          1 0.11443191 0.8855681
24          1 0.11415933 0.8858407
25          1 0.11455826 0.8854417
26          1 0.07305977 0.9269402
27          1 0.12403361 0.8759664
28          1 0.12757631 0.8724237
29          1 0.07948517 0.9205148
30          1 0.10266950 0.8973305
31          1 0.09165839 0.9083416
32          1 0.11902424 0.8809758
33          1 0.11019240 0.8898076
34          1 0.07706342 0.9229366
35          1 0.32844214 0.6715579
36          1 0.26593861 0.7340614
37          1 0.32682189 0.6731781
38          1 0.10437756 0.8956224
39          1 0.14037714 0.8596229
40          1 0.19568200 0.8043180
41          1 0.08930602 0.9106940
42          1 0.15678498 0.8432150
43          1 0.14764123 0.8523588
44          1 0.15020230 0.8497977
45          1 0.19875194 0.8012481
46          1 0.18578030 0.8142197
47          1 0.18178280 0.8182172
48          1 0.08346605 0.9165339
49          1 0.08384006 0.9161599
50          1 0.04726371 0.9527363
51          1 0.22362116 0.7763788
52          1 0.21683711 0.7831629
53          1 0.10234515 0.8976548
54          1 0.16725220 0.8327478
55          1 0.16637279 0.8336272
56          1 0.10877626 0.8912237
57          1 0.17634879 0.8236512
58          1 0.10135119 0.8986488
59          1 0.33005511 0.6699449
60          1 0.19288113 0.8071189
61          1 0.13529139 0.8647086
62          1 0.06457939 0.9354206
63          1 0.09216306 0.9078369
64          1 0.08046230 0.9195377
65          1 0.13186574 0.8681343
66          1 0.13262995 0.8673700
67          1 0.08116288 0.9188371
68          1 0.23393171 0.7660683
69          1 0.13159754 0.8684025
70          1 0.13126033 0.8687397
71          1 0.09292270 0.9070773
72          1 0.13313169 0.8668683
73          1 0.12644576 0.8735542
74          1 0.09558327 0.9044167
75          1 0.07904896 0.9209510
76          1 0.09304245 0.9069576
77          1 0.10437756 0.8956224
78          1 0.16365269 0.8363473
79          1 0.16033989 0.8396601
80          1 0.19119487 0.8088051
81          1 0.13697795 0.8630220
82          1 0.18887223 0.8111278
83          1 0.13074263 0.8692574
84          1 0.12733793 0.8726621
85          1 0.07536575 0.9246342
86          1 0.12317685 0.8768232
87          1 0.22359190 0.7764081
88          1 0.23970990 0.7602901
89          1 0.22778199 0.7722180
90          1 0.22218241 0.7778176
91          1 0.21174840 0.7882516
92          1 0.20597175 0.7940283
93          1 0.17949026 0.8205097
94          1 0.09221401 0.9077860
95          1 0.19809647 0.8019035
96          1 0.15607443 0.8439256
97          1 0.19053074 0.8094693
98          1 0.16328549 0.8367145
99          1 0.09200868 0.9079913
100         1 0.13129289 0.8687071
101         1 0.18231875 0.8176813
102         1 0.09457491 0.9054251
103         1 0.12401607 0.8759839
104         1 0.17455881 0.8254412
105         1 0.13404754 0.8659525
106         1 0.13422306 0.8657769
107         1 0.09737142 0.9026286
108         1 0.10865095 0.8913491
109         1 0.23896016 0.7610398
110         1 0.15558179 0.8444182
111         1 0.15650446 0.8434955
112         1 0.14764123 0.8523588
113         1 0.17621697 0.8237830
114         1 0.15953155 0.8404684
115         1 0.07755115 0.9224489
116         1 0.07808173 0.9219183
117         1 0.08076093 0.9192391
118         1 0.12403361 0.8759664
119         1 0.10233770 0.8976623
120         1 0.08024097 0.9197590
121         1 0.13035665 0.8696433
122         1 0.26424333 0.7357567
123         1 0.38983156 0.6101684
124         1 0.39323426 0.6067657
125         1 0.25829645 0.7417035
126         1 0.15497454 0.8450255
127         1 0.36189444 0.6381056
128         1 0.42847303 0.5715270
129         1 0.32621254 0.6737875
130         1 0.31378827 0.6862117
131         1 0.33002467 0.6699753
132         1 0.31830151 0.6816985
133         1 0.25928379 0.7407162
134         1 0.25131114 0.7486889
135         1 0.24383777 0.7561622
136         1 0.24738575 0.7526143
137         1 0.15116438 0.8488356
138         1 0.12486320 0.8751368
139         1 0.10107576 0.8989242
140         1 0.07743871 0.9225613
141         1 0.09421764 0.9057824
142         1 0.07787742 0.9221226
143         1 0.07855147 0.9214485
144         1 0.07615673 0.9238433
145         1 0.07651467 0.9234853
146         1 0.10132252 0.8986775
147         1 0.10200228 0.8979977
148         1 0.13143459 0.8685654
149         1 0.12032375 0.8796762
150         1 0.08061829 0.9193817
151         1 0.09647035 0.9035297
152         1 0.05009620 0.9499038
153         1 0.09358907 0.9064109
154         1 0.09069001 0.9093100
155         1 0.08222286 0.9177771
156         1 0.19068104 0.8093190
157         1 0.12546738 0.8745326
158         1 0.13574224 0.8642578
159         1 0.17370757 0.8262924
160         1 0.13524828 0.8647517
161         1 0.16579385 0.8342062
162         1 0.16261247 0.8373875
163         1 0.15895375 0.8410462
164         1 0.15762790 0.8423721
165         1 0.12008686 0.8799131
166         1 0.12323162 0.8767684
167         1 0.12463368 0.8753663
168         1 0.09452128 0.9054787
169         1 0.18568488 0.8143151
170         1 0.09007602 0.9099240
171         1 0.09024505 0.9097550
172         1 0.14872807 0.8512719
173         1 0.13755685 0.8624432
174         1 0.05964936 0.9403506
175         1 0.09165839 0.9083416
176         1 0.11426825 0.8857317
177         1 0.09640113 0.9035989
178         1 0.07761104 0.9223890
179         1 0.11415933 0.8858407
180         1 0.07369660 0.9263034
181         1 0.11455826 0.8854417
182         1 0.11549347 0.8845065
183         1 0.22217312 0.7778269
184         1 0.31812087 0.6818791
185         1 0.31680236 0.6831976
186         1 0.29837790 0.7016221
187         1 0.36230110 0.6376989
188         1 0.33956259 0.6604374
189         1 0.27683416 0.7231658
190         1 0.19766810 0.8023319
191         1 0.07635317 0.9236468
192         1 0.11920119 0.8807988
193         1 0.11426473 0.8857353
194         1 0.10013235 0.8998677
195         1 0.09714449 0.9028555
196         1 0.06713170 0.9328683
197         1 0.11578586 0.8842141
198         1 0.14337033 0.8566297
199         1 0.13558092 0.8644191
200         1 0.12632522 0.8736748
201         1 0.12314823 0.8768518
202         1 0.12569993 0.8743001
203         1 0.13085016 0.8691498
204         1 0.10299056 0.8970094
205         1 0.14690072 0.8530993
206         1 0.29819431 0.7018057
207         1 0.08955996 0.9104400
208         1 0.12485863 0.8751414
209         1 0.12782448 0.8721755
210         1 0.07034718 0.9296528
211         1 0.15115853 0.8488415
212         1 0.06709913 0.9329009
213         1 0.11395889 0.8860411
214         1 0.09216306 0.9078369
215         1 0.10166945 0.8983306
216         1 0.11481545 0.8851845
217         1 0.11575023 0.8842498
218         1 0.08281480 0.9171852
219         1 0.13901491 0.8609851
220         1 0.13965323 0.8603468
221         1 0.13624074 0.8637593
222         1 0.11956352 0.8804365
223         1 0.14337033 0.8566297
224         1 0.10028492 0.8997151
225         1 0.12821813 0.8717819
226         1 0.09534970 0.9046503
227         1 0.10515201 0.8948480
228         1 0.09974129 0.9002587
229         1 0.07894352 0.9210565
230         1 0.11443191 0.8855681
231         1 0.11225628 0.8877437
232         1 0.13965323 0.8603468
233         1 0.09168538 0.9083146
234         1 0.12401131 0.8759887
235         1 0.12237684 0.8776232
236         1 0.11875767 0.8812423
237         1 0.09215254 0.9078475
238         1 0.11508566 0.8849143
239         1 0.17090427 0.8290957
240         1 0.09700615 0.9029939
241         1 0.08067917 0.9193208
242         1 0.06083703 0.9391630
243         1 0.11616030 0.8838397
244         1 0.26277669 0.7372233
245         1 0.30305323 0.6969468
246         1 0.23628159 0.7637184
247         1 0.39671185 0.6032881
248         1 0.29362703 0.7063730
249         1 0.28670183 0.7132982
250         1 0.23167011 0.7683299
251         1 0.17949026 0.8205097
252         1 0.17136109 0.8286389
253         1 0.16708084 0.8329192
254         1 0.09087580 0.9091242
255         1 0.11698522 0.8830148
256         1 0.12447268 0.8755273
257         1 0.11905502 0.8809450
258         1 0.22638683 0.7736132
259         1 0.05074071 0.9492593
260         1 0.11959425 0.8804057
261         1 0.14334460 0.8566554
262         1 0.11156019 0.8884398
263         1 0.13933024 0.8606698
264         1 0.13256888 0.8674311
265         1 0.22778199 0.7722180
266         1 0.19722946 0.8027705
267         1 0.12911684 0.8708832
268         1 0.23019777 0.7698022
269         1 0.10996804 0.8900320
270         1 0.16725220 0.8327478
271         1 0.16552204 0.8344780
272         1 0.16389162 0.8361084
273         1 0.13947669 0.8605233
274         1 0.14035583 0.8596442
275         1 0.20273128 0.7972687
276         1 0.20022764 0.7997724
277         1 0.22726327 0.7727367
278         1 0.07832500 0.9216750
279         1 0.20973316 0.7902668
280         1 0.19022591 0.8097741
281         1 0.15386178 0.8461382
282         1 0.09150244 0.9084976
283         1 0.08884672 0.9111533
284         1 0.08767104 0.9123290
285         1 0.14758338 0.8524166
286         1 0.17301946 0.8269805
287         1 0.08003259 0.9199674
288         1 0.11956352 0.8804365
289         1 0.14337033 0.8566297
290         1 0.13965323 0.8603468
291         1 0.09168538 0.9083146
292         1 0.09162929 0.9083707
293         1 0.13883020 0.8611698
294         1 0.12656840 0.8734316
295         1 0.19444977 0.8055502
296         1 0.13261918 0.8673808
297         1 0.27691230 0.7230877
298         1 0.12770632 0.8722937
299         1 0.08453647 0.9154635
300         1 0.09292270 0.9070773
301         1 0.26909079 0.7309092
302         1 0.07602352 0.9239765
303         1 0.07297958 0.9270204
304         1 0.11432671 0.8856733
305         1 0.24797468 0.7520253
306         1 0.16722370 0.8327763
307         1 0.19215796 0.8078420
308         1 0.30256070 0.6974393
309         1 0.17620681 0.8237932
310         1 0.30055572 0.6994443
311         1 0.13586129 0.8641387
312         1 0.10543927 0.8945607
313         1 0.07433626 0.9256637
314         1 0.09088461 0.9091154
315         1 0.07301228 0.9269877
316         1 0.14113919 0.8588608
317         1 0.09807827 0.9019217
318         1 0.11019240 0.8898076
319         1 0.10848432 0.8915157
320         1 0.08545724 0.9145428
321         1 0.09330736 0.9066926
322         1 0.07736709 0.9226329
323         1 0.09647035 0.9035297
324         1 0.09578081 0.9042192
325         1 0.07305977 0.9269402
326         1 0.09680035 0.9031996
327         1 0.07347393 0.9265261
328         1 0.11743043 0.8825696
329         1 0.21418833 0.7858117
330         1 0.08601718 0.9139828
331         1 0.07682676 0.9231732
332         1 0.08177550 0.9182245
333         1 0.08459381 0.9154062
 [ reached getOption("max.print") -- omitted 10657 rows ]
> # Confusion Matrix
> #install.packages("gmodels")
> library(gmodels)
> CrossTable(splited.train2$Label, dl.result$predict,
+            prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
+            dnn = c('actual Labels', 'predicted Labels'))

 
   Cell Contents
|-------------------------|
|                       N |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  10990 

 
              | predicted Labels 
actual Labels |         0 |         1 | Row Total | 
--------------|-----------|-----------|-----------|
            0 |        45 |      1363 |      1408 | 
              |     0.004 |     0.124 |           | 
--------------|-----------|-----------|-----------|
            1 |        36 |      9546 |      9582 | 
              |     0.003 |     0.869 |           | 
--------------|-----------|-----------|-----------|
 Column Total |        81 |     10909 |     10990 | 
--------------|-----------|-----------|-----------|

 
> # accuracy
> table.NN <- table(splited.train2$Label, dl.result$predict)
> nn.accuracy = round(sum(diag(table.NN)/sum(table.NN)),digits=5)
> nn.accuracy
[1] 0.8727
> ###################################### Naive Bayes ##########################################
> 
> splited.train1$Label <- factor(splited.train1$Label)
> splited.train2$Label <- factor(splited.train2$Label)
> 
> #install.packages("e1071")
> library(e1071)
> nb.classifier <- naiveBayes(splited.train1, splited.train1$Label)
> 
> nb.predict <- predict(nb.classifier, splited.train2)
> head(nb.predict)
[1] 1 1 1 1 1 1
Levels: 0 1
> 
> # Confusion Matrix
> library(gmodels)
> CrossTable(nb.predict, splited.train2$Label,
+            prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
+            dnn = c('predicted', 'actual'))

 
   Cell Contents
|-------------------------|
|                       N |
|           N / Col Total |
|-------------------------|

 
Total Observations in Table:  10990 

 
             | actual 
   predicted |         0 |         1 | Row Total | 
-------------|-----------|-----------|-----------|
           0 |      1339 |       405 |      1744 | 
             |     0.951 |     0.042 |           | 
-------------|-----------|-----------|-----------|
           1 |        69 |      9177 |      9246 | 
             |     0.049 |     0.958 |           | 
-------------|-----------|-----------|-----------|
Column Total |      1408 |      9582 |     10990 | 
             |     0.128 |     0.872 |           | 
-------------|-----------|-----------|-----------|

 
> 
> # accuracy
> table.NB <- table(splited.train2$Label, nb.predict)
> nb.accuracy = round(sum(diag(table.NB)/sum(table.NB)),digits=5)
> nb.accuracy
[1] 0.95687
> 
> ###################################### Naive Bayes ##########################################
> #############################################################################################
> #############################################################################################
> #################################### Decision Tree ##########################################
> 
> # Modeling data with decision tree using c50
> #install.packages("C50")
> library(C50)
> dt.classifier <- C5.0(splited.train1[-1], splited.train1$Label)
> 
> # generate predictions for the testing dataset
> dt.predict <- predict(dt.classifier, splited.train2)
> 
> # cross tabulation of predicted versus actual classes
> library(gmodels)
> CrossTable(splited.train2$Label, dt.predict,
+            prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
+            dnn = c('actual', 'predicted'))

 
   Cell Contents
|-------------------------|
|                       N |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  10990 

 
             | predicted 
      actual |         0 |         1 | Row Total | 
-------------|-----------|-----------|-----------|
           0 |       456 |       952 |      1408 | 
             |     0.041 |     0.087 |           | 
-------------|-----------|-----------|-----------|
           1 |       104 |      9478 |      9582 | 
             |     0.009 |     0.862 |           | 
-------------|-----------|-----------|-----------|
Column Total |       560 |     10430 |     10990 | 
-------------|-----------|-----------|-----------|

 
> 
> # accuracy
> table.DT <- table(splited.train2$Label, dt.predict)
> dt.accuracy = round(sum(diag(table.DT)/sum(table.DT)),digits=5)
> dt.accuracy
[1] 0.90391
> 
> #################################### Decision Tree ##########################################
> #############################################################################################
> #############################################################################################
> ######################################## SVM ################################################
> 
> # modeling the data with svm() 
> library(e1071)
> svm.classifier <- svm(Label~.,data=splited.train1)